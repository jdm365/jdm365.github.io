<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Building a Fast Substring Search Index - Jake Mehlman</title>
    <link type=text/css rel=stylesheet href="../style.css">
    <link type=text/css rel=stylesheet href="../prism-gruvbox-dark.css">
<script src="https://s3.amazonaws.com/superjoe/blog-files/prism.js"></script>
</head>
  <body>
    <div>
<a href="../index.html">Jake Mehlman</a> - Building a Fast Substring Search Index (2024 July 21)
</div>
<hr>
<div class="post">
	<h1>Building a Fast Substring Search Index</h1>

<p>
Recently when building an interactive search app, I wanted to support substring search of specific fields.
I needed a low latency approach which scaled up to ~100 million records which typically were fewer than 128 charachters each.
</p>

<p>
The first and most obvious approach was to try a brute force solution.
For this I used the excellent <a href="https://pola.rs/" target=blank>Polars</a> dataframe library.
**For testing puposes here we will use this <a href="https://www.kaggle.com/datasets/mfrye0/bigpicture-company-dataset" target=blank>company dataset</a>
containing ~17 million company names.
Polars has a rust native multi-threaded execution engine and generally will offer significantly higher performance than pandas.
</p>

<pre><code class="language-python">import polars as pl
from time import perf_counter


if __name__ == '__main__':

    FILENAME = "companies-2023-q4-sm.csv"
    df = pl.read_csv(FILENAME)

    ## Convert all names to uppercase
    df = df.with_columns(pl.col("name").str.to_uppercase())

    TEST_QUERIES = [
            "WALMART",
            "AMAZON",
            "MICROSOFT",
            "APPLE",
            "GOOGLE",
            "FACEBOOK",
            "TESLA",
            "NETFLIX",
            "DISNEY",
            "IBM",
            "INTEL"
    ]


    times  = []
    for query in TEST_QUERIES:
        init = perf_counter()
        result = df.filter(
                pl.col("name").str.contains(query)
                )
        time_taken = perf_counter() - init

        times.append(time_taken)

	mean_query_time   = sum(times) / len(times)
	median_query_time = sorted(times)[len(times) // 2]
    print(f"Mean query time:   {mean_query_time:.4f}s")
    print(f"Median query time: {median_query_time:.4f}s")
</code></pre>

<p>Which for a dataframe of 17 million rows returns:
<pre><code>Mean query time:   0.5200s
Median query time: 0.5235s
</code></pre>

<br>
<p>
Fairly impressive but clearly won't suffice for real time search and will not scale to 100 million plus records.
In real terms this gets us:
</p>

<pre><code class="language-python">import os

names = df.select("name")
names.write_csv("names.csv")
file_size_mb = os.path.getsize("names.csv") / (1024 * 1024)
mb_per_second = file_size_mb / mean_query_time
print(f"Polars MB/s: {mb_per_second:.2f}")
</code></pre>

<pre><code>Polars MB/s: 698.25
</code></pre>

<p>
What would be a rough upper bound on how far we might get with a brute force approach?
For this we can look to <a href="https://github.com/BurntSushi/ripgrep" target=blank>ripgrep</a>. One of if not the fastest substring
search tools available.
</p>

<pre><code class="language-python">times  = []
for query in TEST_QUERIES:
	init = perf_counter()
	os.system(f"rg {query} names.csv > /dev/null")
	time_taken = perf_counter() - init

	times.append(time_taken)

mean_query_time   = sum(times) / len(times)
median_query_time = sorted(times)[len(times) // 2]
print(f"Mean query time:   {mean_query_time:.4f}s")
print(f"Median query time: {median_query_time:.4f}s")

file_size_mb = os.path.getsize("names.csv") / (1024 * 1024)
mb_per_second = file_size_mb / mean_query_time
print(f"RipGrep MB/s: {mb_per_second:.2f}")
</code></pre>

<pre><code>RipGrep MB/s: 2753.12
</code></pre>

<p>
Wow that's fast! However this will still only take us so far.
This dataset has roughly <code class="language-python">363MB / 17 million rows ~= 21 bytes per row</code>.
Imposing a 100ms latency cap on all queries, we can calculate that we could at best scale to
<code class="language-python">2,753,120,000 * 0.100 / 21 = 13.1 million rows</code>.
</p>

<p>
We should also bear in mind this is a single user running on an i9 12900k proccessor, utilizing 24 threads with access to 128GB of DDR4 memory.
If we wanted to serve this cheaply in the cloud to multiple users this approach is not feasible above this data scale.
</p>


<h2>Suffix Arrays</h2>

<p>
Given that we know the data ahead of time, perhaps we can precompute a datastructure which would allow faster substring searches.
</p>

<p>
Enter <a href="https://en.wikipedia.org/wiki/Suffix_array#:~:text=In%20computer%20science%2C%20a%20suffix,and%20the%20field%20of%20bibliometrics." target=blank>suffix arrays</a>.
A Suffix array is an auxillary integer array containing the same number of elements as the input string has bytes. 
The string is then lexographically sorted according to its suffixes and the sorted indicies comprise the suffix array.
</p>

<p>
Let's create a suffix array on small data first to better understand.

Take the string: <pre><code class="language-python">"This is a test"</code></pre>

First what are the suffixes here?
<pre><code class="language-python">suffixes = [
	"This is a test",
	"his is a test",
	"is is a test",
	"s is a test",
	" is a test",
	"is a test",
	"s a test",
	" a test",
	"a test",
	" test",
	"test",
	"est",
	"st",
	"t"
]
</code></pre>

Sorting these lexicographically we get:
<pre><code class="language-python">sorted_suffixes = [
	' a test', 
	' is a test', 
	' test', 
	'This is a test', 
	'a test', 
	'est', 
	'his is a test', 
	'is a test', 
	'is is a test', 
	's a test', 
	's is a test', 
	'st', 
	't', 
	'test'
	]
</code></pre>
</p>

<p>
**In practice we will not hold all of the suffixes in memory. This is unnecessary and unfeasible for large strings.
Additionally for long strings we typically can sort the suffixes by only looking at the first few charachters.
For example <code class="language-python">"apple"</code> and <code class="language-python">"ancestor"</code> can be sorted by only looking at the first two charachters,
even if their actual suffix lengths were millions of tokens.
</p>

<p>
Our suffix array would then be the indicies of the sorted suffixes in the original string.
<pre><code class="language-python">suffix_array = [ 7  4  9  0  8 11  1  5  2  6  3 12 13 10 ]</code></pre>
</p>

<p>
How does this allow fast substring quering?
To query a substring we can simply perform a binary search over the suffix array and the string input to find all matching suffixes.
Let's illustrate with an example. For the text above let's say we want to find all occurences of the substring <code class="language-python">"is"</code>.

</p>
<p>
We first will look for the "first" occurence of the substring in our suffix array. Meaning that after lexographic sorting this is the first suffix.
We start our binary search by looking in the middle of our suffix array. Our suffix array has 14 elements so we will look at index 6 first.
Index 6 in our suffix array is 1. This means that we should look at the suffix starting at index 1 or our input string. The suffix at index 1 of
<code class="language-python">"This is a test"</code> is <code class="language-python">"his is a test"</code>. h comes before i lexographically, meaning we undershot. 
Now we set our new low point to be the previous midpoint plus one. So <code class="language-python">low = mid + 1 = 6 + 1 = 7</code>. Our new midpoint is now
<code class="language-python">mid = (low + high) // 2 = (7 + 13) // 2 = 10</code>. So we repeat the process for index 10. Index 10 in our suffix array is 3
and the value of the text at index 3 is <code class="language-python">"s is a test"</code>. <code class="language-python">" test"</code> is lexographically above 
<code class="language-python">"is"</code> meaning we overshot and need to adjust our upper bound to <code class="language-python">high = mid - 1 = 10 - 1 = 9</code>.
Repeating this process we will find that the first occurence of <code class="language-python">"is"</code> is at index 2 of our suffix array and the second
occurence is at index 6, which as we can see are adjacent indexes 7 and 8 in our suffix array. Therefore we will have two occurenences of the substring <code class="language-python">"is"</code>.
Below is a python implementation of this process.
</p>

<pre><code class="language-python">def compare_strings(query: str, suffix: str):

    for idx, char in enumerate(query):
        if idx >= len(suffix):
            ## Query is longer than suffix.
            return 1
        elif char < suffix[idx]:
            ## Character in query is less than character in suffix.
            return -1
        elif char > suffix[idx]:
            ## Character in query is greater than character in suffix.
            return 1

    ## Found exact substring match.
    return 0

def find_first_occurrence(
        query: str,
        text: str,
        suffix_array: np.ndarray
        ):

    low, high = 0, len(suffix_array) - 1
    while low <= high:
        mid = (low + high) // 2
        cmp_result = compare_strings(query, text[suffix_array[mid]:])

        if cmp_result == 0:
            high = mid - 1

        elif cmp_result == -1:
            high = mid - 1

        else:
            low = mid + 1
    
    return low

def find_last_occurrence(
        query: str,
        text: str,
        suffix_array: np.ndarray
        ):

    low, high = 0, len(suffix_array) - 1
    while low <= high:
        mid = (low + high) // 2
        cmp_result = compare_strings(query, text[suffix_array[mid]:])

        if cmp_result == 0:
            low = mid + 1

        elif cmp_result == -1:
            high = mid - 1

        else:
            low = mid + 1
    
    return high

def query(
        query: str,
        text: str, 
        suffix_array: np.ndarray
        ):
    low  = find_first_occurrence(query, text, suffix_array)
    high = find_last_occurrence(query, text, suffix_array)

    if low > high:
        return -1, -1
    return low, high
</code></pre>

<p>
This is the essence of the suffix array.
It allows us to (at query time) find all occurences of a substring in a given text in <code class="language-python">O(log n)</code> time.
</p>

<p> Two questions remain:
<ol>
	<li>How should we build a suffix array for our dataset?</li>
	<li>How can we query a subset of the text in a suffix array?</li>
</ol>
</p>


<h2>Building a Suffix Array</h2>
<p>
As previously mentioned, the problem of building a suffix array is just a matter of sorting the suffixes of a string.
Therefore efficiently creating a suffix array is largely just determining the ideal sorting algorithm.
</p>

<p>
Let's first try a naive approach by using c++'s standard library's sort. This is typically implemented as a quicksort or mergesort O(nlogn).
</p>

<pre><code class="language-c">void construct_suffix_array_default(
	const char* str,
	uint32_t* suffix_array,
	uint32_t n
) {
	// Construct simple suffix array from str
	for (uint64_t i = 0; i < n; ++i) {
		suffix_array[i] = i;
	}

	// Sort suffix array
	std::sort(
		suffix_array,
		suffix_array + n,
		[&](int a, int b) {
			return strcmp(str + a, str + b) < 0;
		}
	);
}
</code></pre>

<p>
Running again on the 363MB dataset of company names, this doesn't finish within 12 hours.
By imposing a restriction however we can greatly improve performance. Users searching for companies will rarely if ever search for long substrings. Concretely, we can limit the suffixes to only consider the first ~32 charachters and stop sorting strings longer than that. Imposing that restriction we get the code below and greatly improve performance.
</p>

<pre><code class="language-c">void construct_truncated_suffix_array_default(
	const char* str,
	uint32_t* suffix_array,
	uint32_t max_suffix_length,
	uint32_t n
) {
	// Construct simple suffix array from str
	for (uint64_t i = 0; i < n; ++i) {
		suffix_array[i] = i;
	}

	// Sort suffix array
	std::sort(
		suffix_array,
		suffix_array + n,
		[&](int a, int b) {
			return strncmp(str + a, str + b, max_suffix_length) < 0;
		}
	);
}
</code></pre>
We now get a construction time of:
<pre><code>Elapsed time construction default:  120.482775 seconds</code></pre>

<p>
Much better! Clearly however this won't continue to scale to larger datasets of size 100 million plus records. We would like an algorithm which:
</p>
<ol>
	<li>Can be constructed in approximately linear time</li>
	<li>Can be parallelized</li>
	<li>Uses minimal auxillary memory</li>
</ol>
</p>

<h2>Linear Time Suffix Array Construction</h2>

<p>
There are several linear time suffix array construction algorithms. We will focus on <a href="https://www.cs.helsinki.fi/u/tpkarkka/publications/jacm05-revised.pdf" target=blank>SA-IS</a> and the implementation <a href="https://github.com/IlyaGrebnov/libsais" target=blank>libsais</a>. This algorithm is a parallelizable linear time suffix array construction algorithm which only uses 5N auxilliary memory (for the suffix array 4N, and the text itself N). This is also the code used by <a href="https://github.com/Intsights/PySubstringSearch" target=blank>PySubstringSearch</a> to construct their suffix arrays for substring searching (this implementation is very similar to theirs).
</p>

<p>
I will not go into the details of this algorithm here as this is beyond the scope of this article.
</p>

<img src="./meme0.png" alt="meme0" width="400"/>

<p>
We will however benchmark it against our previous implementation.
</p>

<pre><code class="language-c">void bench_libsais(const char* filename) {
	auto start = std::chrono::high_resolution_clock::now();

	char* buffer = nullptr;
	uint64_t buffer_size;
	read_text_into_buffer(filename, &buffer, &buffer_size);

	auto end = std::chrono::high_resolution_clock::now();
	std::chrono::duration<double> elapsed_read = end - start;

	int32_t* suffix_array = (int32_t*)malloc(buffer_size * sizeof(int32_t));
	int32_t* freq_table   = (int32_t*)malloc(256 * sizeof(int32_t));
	int32_t status = libsais(
		(const uint8_t*)buffer,
		suffix_array,
		(int32_t)buffer_size,
		(int32_t)0,
		freq_table
	);
	end = std::chrono::high_resolution_clock::now();
	std::chrono::duration<double> elapsed_truncated = end - start;

	printf("File read time:                     %f seconds\n", elapsed_read.count());
	printf("Elapsed time construction libsais:  %f seconds\n\n\n\n", elapsed_truncated.count());

	free(suffix_array);
	free(freq_table);
	free(buffer);
}
</code></pre>

<p>
Which gives us a much improved construction time of:
</p>
<pre><code>Elapsed time construction libsais:  12.558940 seconds</code></pre>

<p>
Additionally this is in linear time. But can we do better given our <code>max_suffix_length</code> restriction?
</p>

<h2>Bucket Sorting</h2>
<p>
Bucket sorting is a far more simple algorithm than SA-IS which also has the benefits of being parallelizable and linear time (O(n + k) where k is 256 in our use case). It does however require 4N more memory than SA-IS (for the suffix array, a copy of the suffix array, and the text itself).
</p>

<p>
Let's benchmark the performance of this method.
</p>

<pre><code class="language-c">void construct_truncated_suffix_array(
	const char* str,
	uint32_t* suffix_array,
	uint32_t n,
	uint32_t max_suffix_length,
	bool skip_newline
) {
	uint64_t n = suffix_array.size();

	max_suffix_length = std::min(max_suffix_length, (uint32_t)n);
	alignas(64) uint32_t* temp_suffix_array = (uint32_t*)malloc(n * sizeof(uint32_t));

	// Construct simple suffix array from str
	#pragma omp parallel for
	for (uint64_t i = 0; i < n; ++i) {
		suffix_array[i] = i;
	}

	recursive_bucket_sort(
		str,
		suffix_array.data(),
		temp_suffix_array,
		n,
		n,
		max_suffix_length,
		0,
		skip_newline
	);

	free(temp_suffix_array);
}
</code></pre>

<p>
We parallelize the bucket sort by using openmp and sort sufficiently small buckets (<32 elements) with an insertion sort. 
</p>
<pre><code class="language-c">void recursive_bucket_sort(
	const char* str,
	uint32_t* suffix_array,
	uint32_t* temp_suffix_array,
	int string_length,
	uint64_t n,
	int max_depth,
	int current_depth,
	bool skip_newline
) {
	// Base case
	if (current_depth == max_depth) {
		return;
	}

	if (n < 32) {
		// Do insertion sort
		for (uint64_t i = 1; i < n; ++i) {
			int j = i;
			// while (j > 0 && strncmp(str + suffix_array[j] + current_depth, str + suffix_array[j - 1] + current_depth, max_depth) < 0) {
			while (j > 0 && strncmp_128(str + suffix_array[j] + current_depth, str + suffix_array[j - 1] + current_depth, max_depth) < 0) {
				std::swap(suffix_array[j], suffix_array[j - 1]);
				--j;
			}
		}
		return;
	}

	// Do bucket sort of first char. Then in each bucket, skip
	// current_depth chars and do bucket sort of next char.

	constexpr int NUM_BUCKETS 	   		 = 256;
	uint32_t _buckets[NUM_BUCKETS] 	     = {0};
	uint32_t _bucket_starts[NUM_BUCKETS] = {0};

	for (uint64_t i = 0; i < n; ++i) {
		int char_idx = suffix_array[i] + current_depth;
		uint8_t char_val = (uint8_t)str[char_idx];
		++_buckets[char_val];
	}

	uint64_t offset = 0;
	for (size_t i = 0; i < NUM_BUCKETS; ++i) {
		_bucket_starts[i] = offset;
		offset += _buckets[i];
	}

	memcpy(temp_suffix_array, suffix_array, n * sizeof(uint32_t));

	for (uint64_t i = 0; i < n; ++i) {
		int char_idx = suffix_array[i] + current_depth;
		uint8_t char_val = (uint8_t)str[char_idx];
		temp_suffix_array[_bucket_starts[char_val]] = suffix_array[i];
		++_bucket_starts[char_val];
	}

	memcpy(suffix_array, temp_suffix_array, n * sizeof(uint32_t));

	// Recalculate bucket starts
	offset = 0;
	for (size_t i = 0; i < NUM_BUCKETS; ++i) {
		_bucket_starts[i] = offset;
		offset += _buckets[i];
	}

	// Recursively sort each bucket
	if (current_depth == 0) {
		#pragma omp parallel for schedule(guided)
		for (int i = 0; i < 255; ++i) {

			// Don't sort newline characters
			if (skip_newline && (char)i == '\n') {
				continue;
			}

			int bucket_start = _bucket_starts[i];
			int bucket_end   = _bucket_starts[i + 1];
			if (bucket_end - bucket_start <= 1) {
				continue;
			}

			recursive_bucket_sort(
				str,
				suffix_array + bucket_start,
				temp_suffix_array + bucket_start,
				string_length,
				bucket_end - bucket_start,
				max_depth,
				current_depth + 1,
				skip_newline
			);
		}
		return;
	}

	for (int i = 0; i < 255; ++i) {
		if (skip_newline && (char)i == '\n') {
			continue;
		}

		int bucket_start = _bucket_starts[i];
		int bucket_end   = _bucket_starts[i + 1];
		if (bucket_end - bucket_start <= 1) {
			continue;
		}

		recursive_bucket_sort(
			str,
			suffix_array + bucket_start,
			temp_suffix_array + bucket_start,
			string_length,
			bucket_end - bucket_start,
			max_depth,
			current_depth + 1,
			skip_newline
		);
	}
}
</code></pre>

<p>
This gives us an improved construction time of:
</p>
<pre><code>Elapsed time construction bucket: 7.790694 seconds</code></pre>

<p>
Even extending the max_suffix_length to 128 we get a construction time of:
</p>
<pre><code>Elapsed time construction bucket: 7.852674 seconds</code></pre>

<p>
This recursive bucket sorting approach will only start to suffer when certain buckets become very large. An example might be in a piece of text where there are a lot of spaces for paragraph breaks or for other reasons. In this case the bucket sort will become less efficient and the performance will degrade. However by cutting off the suffixes at a reasonable length we can avoid this issue and as shown actually get improved construction times over SA-IS.
</p>

<h2>Limiting Maximum Memory Usage</h2>

<p>
While slightly faster, this approach does require more intermediate RAM as highlighted before. How can we limit the maximum overall memory usage of this algorithm to enable scaling to significantly larger datasets?
The solution used here is just to read the file in chunks and subsequently <code>num_chunks</code> suffix arrays <code>num_chunks</code> times per query. Heuristically choosing <code>2GB</code> file chunks to read at once
(need to keep file chunks below 4GB if we still want to use 4 byte uint32_t's instead of 8 byte uint64_t's) we will see maximum possible memory usage of 
<p>
<pre><code class="language-python">N = 2
Peak_memory_usage = N (original_text) + 4N (suffix_array) + 4N (temp_suffix_array) = 9N = 18GB
</code></pre>

<p>
N can of course be set to smaller values to accomodate hardware limitations. In practice however memory usage will be lower than this 9N when reading and parsing say a csv file.
We will read <code>2GB</code> from the file but only the text of the relevant column will be counted towards the N bytes.
</p>

<h2>Efficiently Querying CSV Files</h2>
<p>
We have successfully constructed our suffix array! We can now find all substring matches in our original text with <code>num_chunks</code> different binary searches over at most <code>2GB</code> text chunks. We will get to benchmarks shortly but we will see that this can typically be done in a handful of microseconds if the suffix arrays and text are held in memory, and at most milliseconds if we resort to memory mapping or seeking through the files on disk (assuming SSD).
</p>

<p>
Before that however we need to determine how we can actually use our suffix arrays to search over csv files, where there are other fields and other data not used for suffix array construction. We will focus on the <a href="https://datatracker.ietf.org/doc/html/rfc4180" target=blank>RFC4180</a> standard of CSV files which encloses fields in quotes which have escape charachters such as line breaks.
</p>

<p>
The first issue we have to address is how to have our suffix array point us to the right position of the csv file? Instead of our suffix array pointing us to the correct position of our text buffer (consisting only of searchable fields), we need it instead to point to specific positions in the file where the indexed text is located. This unfortunately means we will need another <code>4N</code> temporary array mapping from text_buffer_idx to file pos, which we will use after suffix array chunk construction to remap the suffix array indices to file position indices.
</p>

<pre><code>N = 2
New_peak_memory_usage = N (original_text) + 4N (suffix_array) + 4N (temp_suffix_array) + 4N (suffix_array_file_mapping) = 13N = 26GB
</code></pre>

<p>
<i>
Note: While this solution does work, the extra memory and compute required to get around the quirks of this file format are quite inelegant. It is worth considering if writing an auxilliary data structure instead of directly reading off of CSV's would be a nicer solution.
</i>
</p>

<p>
After construction and mapping of the suffix array to the file positions, we are left only with <code>4N</code> additional data
<pre><code>N = Total text bytes
Result_memory_space = 4N (suffix_array) = 4N
</code></pre>
where N is not the size of the CSV file, but the size of the text in the CSV file (plus a constant factor for storing additional line breaks).
</p>

<p>
Let's return to our company dataset. If we want to hold the suffix array and the suffix text in memory for the company names we will need:
<pre><code>N = 363MB
Memory_usage = 4 * 363MB = 1.45GB
</code></pre>
despite the actual file itself being 2GB.
</p>

<p>
This is actually quite feasible and may well scale up to 100 million+ records of this sort assuming reasonable hardware with at least <code>8GB</code> of RAM. If we keep the data on disk we will likely just be bottlenecked by disk seek speeds. As a rough estimation given an SSD seek speed of 100 microseconds we can exect <code>2 * log2(N) = 2 * log2(363MB) = 56</code> seeks to find the correct position in the file. This would give us a query time of <code>56 * 100 microseconds = 5.6 milliseconds</code>. In practice (for reasons we will see shortly) times may be even shorter.
</p>

<p>
Before moving on to benchmarks we must first address one final and incredibly annoying issue related to RFC4180 encoding. That is the issue of determining proper unescaped line breaks. When seeking to an arbitrary position within an RFC4180 CSV file, it is impossible to know if you are within an escaped (quoted) field, thus scanning forward or backward from a position to find an unescaped line break cannot be done.
</p>

<p>
To attempt to handle this issue with minimal additional memory overhead, we will store a bitarray of length <code>N bits</code> indicating
that the charachter at that position of the suffix array is within a quoted field. With this knowledge we can successfully seek to the relevant position of the file as determined from our suffix array query, and seek backwards and forwards to the record's boundaries. This slightly increased our total memory usage to: <code>4N + N/8</code> bytes.
</p>

<p>We can now define our binary search function which will find the first and last occurence of the query substring within the suffix array defined over the relevant columns.</p>

<pre><code class="language-c">pair_u32 get_substring_positions_file(
	FILE* file,
	const SuffixArray_struct* suffix_array,
    const char* substring
) {
    uint32_t m 	   = strlen(substring);
    uint32_t first = 0;
    uint32_t last  = suffix_array->n - 1;

    uint32_t start = UINT32_MAX;
	uint32_t end   = UINT32_MAX;

	uint32_t cmp_length = min(m, suffix_array->max_suffix_length);

	char* line = (char*)malloc((cmp_length) * sizeof(char));

    // Binary search for the first occurrence of the substring
    while (first <= last) {
        uint32_t mid = (first + last) / 2;
		fseek(
				file,
				suffix_array->global_byte_start_idx + (uint64_t)suffix_array->suffix_array[mid],
				SEEK_SET
				);
		fread(line, 1, cmp_length, file);

		for (uint32_t i = 0; i < cmp_length; ++i) {
			line[i] = tolower(line[i]);
		}

		int cmp = strncmp(line, substring, cmp_length);

        if (cmp < 0) {
            first = mid + 1;
        }
        else {
            last  = mid - 1;
			if (cmp == 0) {
				start = mid;
			}
        }
    }

    if (start == UINT32_MAX) {
		free(line);
		pair_u32 result = {UINT32_MAX, UINT32_MAX};
        return result;
    }

    // Reset for searching the last occurrence
    first = start;
	last  = suffix_array->n - 1;
    while (first <= last) {
        uint32_t mid = (first + last) / 2;
		fseek(
				file,
				suffix_array->global_byte_start_idx + (uint64_t)suffix_array->suffix_array[mid],
				SEEK_SET
				);
		fread(line, 1, cmp_length, file);

		for (uint32_t i = 0; i < cmp_length; ++i) {
			line[i] = tolower(line[i]);
		}

		int cmp = strncmp(line, substring, cmp_length);
        if (cmp > 0) {
            last = mid - 1;
        }
        else {
            first = mid + 1;
			if (cmp == 0) {
				end = mid;
			}
        }
    }

	free(line);
	
	pair_u32 result = {start, end};
	return result;
}
</code></pre>

<p>We can also define the logic to actually find the relevant records in the csv file and return them as an array of strings.</p>

<pre><code class="language-c">uint32_t get_matching_records_file(
	const char* filename,
	const SuffixArray_struct* suffix_array,
	const char* substring,
	uint32_t k,
	char** matching_records
) {
	FILE* file = fopen(filename, "r");
	if (file == NULL) {
		printf("Error: File not found\n");
		exit(1);
	}

	pair_u32 match_idxs = get_substring_positions_file(
			file,
			suffix_array,
			substring
			);

	if ((match_idxs.first == UINT32_MAX) || (match_idxs.second == UINT32_MAX) || (match_idxs.first == match_idxs.second + 1)) {
		return 0;
	}

	uint32_t num_matches = min(k, match_idxs.second - match_idxs.first + 1);


	fseek(file, 0, SEEK_END);
	uint64_t total_bytes_file = ftell(file);

	assert(suffix_array->global_byte_start_idx < total_bytes_file);
	assert(suffix_array->global_byte_end_idx  == total_bytes_file);
	assert(suffix_array->is_quoted_bitflag->buffer_bit_idx == suffix_array->n);

	uint32_t match_idx = 0;
	for (uint32_t i = match_idxs.first; i < match_idxs.first + num_matches; ++i) {
		uint8_t is_quoted 	 = get_buffer_bit(suffix_array->is_quoted_bitflag, i);
		uint64_t byte_offset = (uint64_t)suffix_array->suffix_array[i] + 
										 suffix_array->global_byte_start_idx;

		uint32_t num_chars_back  = rfc4180_seek_backward_newline(
				file, 
				byte_offset, 
				is_quoted,
				total_bytes_file
				);
		uint32_t num_chars_ahead = rfc4180_seek_forward_newline(
				file, 
				byte_offset, 
				is_quoted, 
				total_bytes_file
				);

		char* record = (char*)malloc(num_chars_back + num_chars_ahead + 1);
		fseek(file, byte_offset - num_chars_back, SEEK_SET);
		fread(record, 1, num_chars_back + num_chars_ahead, file);
		record[num_chars_back + num_chars_ahead] = '\0';

		matching_records[match_idx++] = record;
	}
	fclose(file);

	return num_matches;
}
</code></pre>

<p>
TODO: <i>Have managed file handles which stay open while index is queryable. Avoid unnecessary syscalls.</i>
</p>


<h2>Benchmarks and Additional Optimizations</h2>

<h3>Consturction</h3>
<p><b>Note on all measurements: </b>These are taken on my desktop with the hardware mentioned above. The environment is not particularly strongly controlled and measurements will vary from run to run so you may see some slightly different results under the same conditions. I will try not to cherry pick results and will run benchmark tests to confirm that the results I get and provide are representative. Generally I am more interested in relative performance and not absolute performance.</p>

<p>Additional optimizations:</p>
<ol>
	<li><b>Stop sorting on line breaks:</b> Both for record breaks and newlines which occur in the text, we do not continue sorting accross these boundaries. This will truncate the amount of total sorting operations and bucket constructions a fair bit. On the company dataset without newline truncation I get a construction time of <code>9.764</code> seconds, and with newline truncation I get <code>8.431</code> seconds.</li>
	<li><b>Custom avx strncmp (minimal):</b> Compare suffixes 16 bytes at a time in the final insertion sort part of the suffix array construction. With custom strncmp_128 <code>8.806</code> seconds, with <code>8.178</code> seconds.</li>
	<li><b>Thread scheduling (TODO): </b> Currently my threads are being dispatched over all 256 buckets of my top level recursive_bucket_sort using OpenMP. I have experimented dispatching the threads under static, dynamic, and guided modes and found the best most reliable performance to come from the dynamic mode. When dispatching the threads however I do know beforehand the weight of each workload by the size of each bucket (given by charachter frequency distribution). Therefore it stands to reason that I could dispatch the threads in proportion to the workload. This is listed as a TODO for now.</li>
</ol>

<h3>Querying</h3>

<p>
Query times are quickly dominated by seeking and fetching the individual records. For this reason we can see that query times increase linearly with <code>k</code>, the number of records returned.
</p>
<!-- <img src=""> -->

<p>
Here are query times for this implementation and the similar library PySubstringSearch for this task. (Ours fetches csv records over the file itself. PySubstringSearch writes a new buffer only with the search column's text, and only returns the search column's text.) Clearly the requirement for very fast (sub-millisecond) substring search over potentially hundereds of millions of records has been satisfied. A downside however is that relevance is not natively supported in this kind of search index. It will have to be done on all records with matching substrings if desired which may result in many row fetches and thus become more expensive than a standard BM25 implementation say.
However for simple substring matching this method seems to be near the best we can do.
</p> 

<p>
At the time of writing, I have not yet published this library to pypi. I first want to add support for multi-column search, support for search with the suffix array on disk, and speed up the file reader. However for the current version feel free to check out the <a href="">repo</a> and you can install with a 
<pre><code class="language-bash">pip install .</code></pre>
then use with the python interface as so
<pre><code class="language-bash">from suffix_array import SuffixArray

CSV_FILE   = "companies.csv"
SEARCH_COL = "name"

## Creates and builds the index from documents.
suffix_array = SuffixArray(max_suffix_length=32)
suffix_array.construct_truncated_suffix_array_from_csv(CSV_FILE, SEARCH_COL)

## Query the index.
## Returns the documents that contain the query substring in a list.
records = suffix_array.query_records("the quick brown fox")
</code></pre>
</p>





<h2>Credits</h2>

<hr>
<p>As mentioned, this implementation is very similar to <a href="https://github.com/Intsights/PySubstringSearch" target=blank>PySubstringSearch</a> only designed to work off of
and directly index into csv files. Check out their library with the link provided.</p>

<p><a href="../index.html">Home Page</a> |
<a href="https://github.com/jdm365/" target=blank>GitHub</a></p>
</body>
</html>
